= Stream Data from Google Cloud Storage
:sectnums:

You can create a data connector between TigerGraph's internal Kafka server and your GCS service with a specified topic.
The connector will stream data from the data source in your GCS buckets to TigerGraph's internal Kafka cluster.
You can then create and run a loading job to load data from Kafka into the graph store using the xref:kafka-loader/index.adoc[Kafka loader].

== Prerequisites

* Your source data files are stored in Google Cloud Storage buckets.
* You have link:https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating[generated a key that has access to your source data from your Google Cloud Platform service account].

== Procedure

=== Specify connector configurations
The connector configurations provide the following information:

* Connector class
* Your GCP service account credentials
* Information on how to parse the source data
* Mapping between connector and source file

==== Specify connector class
The connector class indicates what type of connector the configuration file is used to create.
Connector class is specified by the `connector.class` key.
For connecting to GCS, its value is set to `com.tigergraph.kafka.connect.filesystem.gcp.GCPSourceConnector`.

==== Provide GCP account credentials
You need to provide following information for the connector to access the files in your GCS bucket.
You should be able to find all items on this list in your link:https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating[service account key]:

* `file.reader.settings.fs.gs.auth.service.account.email`: The email address associated with the service account
* `file.reader.settings.fs.gs.auth.service.account.private.key.id`: The private key ID associated with the service account used for GCS access.
* `file.reader.settings.fs.gs.auth.service.account.private.key`: The private key associated with the service account used for GCS access.
* `file.reader.settings.client_email`:
The service account email.
* `file.reader.settings.fs.gs.project.id`: Google Cloud Project ID with access to GCS buckets.
* `file.reader.settings.fs.gs.auth.service.account.enable`: Whether to create objects for the parent directories of objects with `/` in their path. For example, creating `gs://bucket/foo/` upon deleting or renaming `gs://bucket/foo/bar`.
* `file.reader.settings.fs.gs.impl`: The FileSystem for gs: (GCS) uris.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem`.
* `file.reader.settings.fs.AbstractFileSystem.gs.impl`: The AbstractFileSystem for `gs:` (GCS) URIs. Only necessary for use with Hadoop 2.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS`.

include::partial$parsing-rules.adoc[]


==== Map source file to connector
The below configurations are required:

|===
|Name |Description |Default

| `name`
| Name of the connector.
| None. Must be provided by the user.

| `topic`
| Name of the topic to create in Kafka.
| None. Must be provided by the user.

|`tasks.max`
|The maximum number of tasks which can run in parallel.
|1

|`file.uris`
|The path(s) to the data files on Google Cloud Storage.
The URI may point to a CSV file, a zip file, a gzip file, or a directory
|None.
Must be provided by the user.
|===

==== Example

Below is an example configuration:

[,text]
----
connector.class=com.tigergraph.kafka.connect.filesystem.gcp.GCPSourceConnector
file.reader.settings.fs.gs.auth.service.account.email=gcsconnect@example.iam.gserviceaccount.com
file.reader.settings.fs.gs.auth.service.account.private.key.id=<example>
file.reader.settings.fs.gs.auth.service.account.private.key="<example>"
file.reader.settings.client_email="gcsconnect@example.iam.gserviceaccount.com"
file.reader.settings.fs.gs.project.id="example"
file.reader.settings.fs.gs.auth.service.account.enable=true
file.reader.settings.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
file.reader.settings.fs.AbstractFileSystem.gs.impl="com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"

mode=eof
file.regexp=".*"
file.recursive=true
file.reader.type=text
file.reader.batch.size=10000
file.reader.text.eol="\\n"
file.reader.text.header=true
file.reader.text.archive.type=auto
file.reader.text.archive.extensions.tar=tar
file.reader.text.archive.extensions.zip=zip
file.reader.text.archive.extensions.gzip=gz
file.reader.text.archive.extensions.tar.gz=tar.gz,tgz

[connector_person]
name = fs-person-demo-104
tasks.max=10
topic=person-demo-104
file.uris=gs://example/p.csv

[connector_friend]
name = fs-friend-demo-104
tasks.max=10
topic=friend-demo-104
file.uris=gs://example/f.csv
----

include::partial$create-connector.adoc[]

include::partial$create-data-source.adoc[]

include::partial$create-loading-job-kafka.adoc[]

include::partial$loading-job-example-csv.adoc[]



=== Stream data source updates

To load updates to the data source after you've created the connectors, you need to delete the connector and recreate another connector.

In the future, the streaming data connector will support automatically scanning for updates and stream data to TigerGraph.